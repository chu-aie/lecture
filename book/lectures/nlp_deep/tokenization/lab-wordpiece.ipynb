{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfdc61fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Subword Tokenization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e3e6131",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### WordPiece in Practice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eb21ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, vocab size: 79\n",
      "Best pair: ('8', '##9')\n",
      "New token: 89\n",
      "Iteration: 100, vocab size: 179\n",
      "Best pair: ('##1', '##6')\n",
      "New token: ##16\n",
      "Iteration: 200, vocab size: 279\n",
      "Best pair: ('4', '##14')\n",
      "New token: 414\n",
      "Iteration: 300, vocab size: 379\n",
      "Best pair: ('n', '##225')\n",
      "New token: n225\n",
      "Iteration: 400, vocab size: 479\n",
      "Best pair: ('50', '##0')\n",
      "New token: 500\n",
      "Iteration: 500, vocab size: 579\n",
      "Best pair: ('57', '##0')\n",
      "New token: 570\n",
      "Iteration: 600, vocab size: 679\n",
      "Best pair: ('e', '##q')\n",
      "New token: eq\n",
      "Iteration: 700, vocab size: 779\n",
      "Best pair: ('exx', '##o')\n",
      "New token: exxo\n",
      "Iteration: 800, vocab size: 879\n",
      "Best pair: ('quantif', '##y')\n",
      "New token: quantify\n",
      "Iteration: 900, vocab size: 979\n",
      "Best pair: ('lymp', '##h')\n",
      "New token: lymph\n",
      "Final vocab size: 1000\n"
     ]
    }
   ],
   "source": [
    "from ekorpkit.tokenizers.wordpiece import WordPieceTokenizer\n",
    "\n",
    "wp = WordPieceTokenizer()\n",
    "\n",
    "vocab_size = 1000\n",
    "wp.fit(texts, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8822422f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['company']\n",
      "['compani', '##e', '##s']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(wp.encode_word(\"company\"))\n",
    "print(wp.encode_word(\"companies\"))\n",
    "print(wp.encode_word(\"회사\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d10deb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', '##n', '##v', '##e', '##s', '##t', '##m', '##e', '##n', '##t', '###', '###', 'opportuniti', '##e', '##s', '###', '###', 'i', '##n', '###', '###', 'th', '##e', '###', '###', 'company', '###', '###']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = wp.tokenize(\"Investment opportunities in the company\")\n",
    "print(tokenized_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b31328f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### WordPiece Step-by-Step Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e0fd5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 7847\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def pre_tokenize(text, lowercase=True):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.split(\" \")\n",
    "\n",
    "\n",
    "def initialize_vocab(texts, lowercase=True):\n",
    "    vocab = defaultdict(int)\n",
    "    for text in texts:\n",
    "        words = pre_tokenize(text, lowercase)\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "word_freqs = initialize_vocab(texts)\n",
    "print(\"Number of words: {}\".format(len(word_freqs.keys())))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46fe7cf7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The alphabet is the unique set composed of all the first letters of words, and all the other letters that appear in words prefixed by ##:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "486e42fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##0', '##1', '##2', '##3', '##4', '##5', '##6', '##7', '##8', '##9', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##j', '##k', '##l', '##m', '##n', '##o', '##p', '##q', '##r', '##s', '##t', '##u', '##v', '##w', '##x', '##y', '##z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "characters = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in characters:\n",
    "        characters.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in characters:\n",
    "            characters.append(f\"##{letter}\")\n",
    "\n",
    "characters.sort()\n",
    "\n",
    "print(characters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b913df4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Add the special tokens used by the model at the beginning of that vocabulary. In the case of BERT, it’s the list [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20a72d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + characters.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9c4a680",
   "metadata": {},
   "source": [
    "Split each word, with all the letters that are not the first prefixed by ##:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b95130df",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3494fddb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "A function to compute the score of each pair:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3b494b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73134672",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', '##n'): 3.065532997859911e-05\n",
      "('##n', '##v'): 5.217525332521506e-06\n",
      "('##v', '##e'): 2.2967510416892118e-05\n",
      "('##e', '##s'): 6.2108678847586545e-06\n",
      "('##s', '##t'): 7.931201514160114e-06\n",
      "('##t', '##i'): 8.905730802064189e-06\n"
     ]
    }
   ],
   "source": [
    "pair_scores = compute_pair_scores(splits)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "150b06f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Find the pair with the highest score:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8036f196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('8', '##9') 0.0004752098843655948\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_score = None\n",
    "for pair, score in pair_scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "        best_pair = pair\n",
    "        max_score = score\n",
    "\n",
    "print(best_pair, max_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08498fdf",
   "metadata": {},
   "source": [
    "So the first merge to learn is (`8`, `##9`) -> `89`. Add it to the vocabulary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f633bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.append(\"89\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe293c5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "To continue, we need to apply that merge in our splits dictionary. A function for this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84dbb5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85d4ebe0",
   "metadata": {},
   "source": [
    "And we can have a look at the result of the first merge:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d624b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['89', '##2', '##0']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = merge_pair(\"8\", \"##9\", splits)\n",
    "splits[\"8920\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "048d0b8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Now we have everything we need to loop until we have learned all the merges we want. For example, we can loop until we have a vocabulary of size 1000:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e84fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "    best_pair, max_score = \"\", None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    new_token = (\n",
    "        best_pair[0] + best_pair[1][2:]\n",
    "        if best_pair[1].startswith(\"##\")\n",
    "        else best_pair[0] + best_pair[1]\n",
    "    )\n",
    "    vocab.append(new_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc976956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 tokens: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##0', '##1', '##2', '##3', '##4']\n",
      "Last 50 tokens: ['thompso', 'accompan', 'accompany', 'accompani', 'thompson', 'compa', 'compan', 'company', 'compani', 'compar', 'compac', 'compas', 'compass', 'compact', 'employm', '##mpl', '##mply', '##imply', 'simply', '##amp', 'camp', 'ramp', 'hamp', 'damp', '##vamp', '##ymp', 'lymp', 'lymph', '##lymp', 'olymp', 'olympi', 'olympic', 'lympho', 'lymphom', 'lymphoma', 'campa', 'campai', 'campaig', 'campaign', '##rump', 'trump', 'bump', 'gump', 'pump', 'pumps', '##lump', 'slump', '##sump', '##sumpt', '##sumpti']\n"
     ]
    }
   ],
   "source": [
    "print(\"First 10 tokens: {}\".format(vocab[:10]))\n",
    "print(\"Last 50 tokens: {}\".format(vocab[-50:]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4118aea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Encoding is done by finding the biggest subword in the vocabulary that is in the word, and splitting on it. Iterating on the word until it is empty:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a0f6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e55dbedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['company']\n",
      "['compani', '##e', '##s']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"company\"))\n",
    "print(encode_word(\"companies\"))\n",
    "print(encode_word(\"회사\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f400b6c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "To tokenize a sentence, we can apply this function to each word:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "806bbc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    words = pre_tokenize(text)\n",
    "    encoded_words = [encode_word(word) for word in words]\n",
    "    return sum(encoded_words, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50a9afe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', '##n', '##v', '##e', '##s', '##t', '##m', '##e', '##n', '##t', 'opportuniti', '##e', '##s', 'i', '##n', 'th', '##e', 'company']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenize(\"Investment opportunities in the company\")\n",
    "print(tokenized_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6806c658",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b97666b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The procedure of obtaining the vocabulary V with a desired size.\n",
    "\n",
    "1. Initialize a reasonably big seed vocabulary.\n",
    "2. Define a desired vocabulary size.\n",
    "3. Optimize the subword occurrence probabilities using the EM algorithm by fixing the vocabulary.\n",
    "4. Compute the loss for each subword.\n",
    "   - The loss of a subword depicts the decrement in the aforementioned likelihood $L$ when that subword is removed from the vocabulary.\n",
    "5. Sort the subwords by loss and keep the top n% of subwords.\n",
    "   - Keep the subwords with a single character to avoid the out of vocabulary problem.\n",
    "6. Repeat step 3 to 5 until it reaches the desired vocabulary size defined in step 2.\n",
    "\n",
    "The most common way to prepare the seed vocabulary is to use the most frequent substrings and characters in the corpus. This unigram language model based subword segmentation consists of characters, subwords and words.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e69411c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Implementation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab9abd5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Unigram Step-by-Step Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7150dafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 7847\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def pre_tokenize(text, lowercase=True):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.split(\" \")\n",
    "\n",
    "\n",
    "def initialize_vocab(texts, lowercase=True):\n",
    "    vocab = defaultdict(int)\n",
    "    for text in texts:\n",
    "        words = pre_tokenize(text, lowercase)\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "word_freqs = initialize_vocab(texts)\n",
    "print(\"Number of words: {}\".format(len(word_freqs.keys())))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69e0d190",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Then, we need to initialize our vocabulary to something larger than the vocabulary size we want.\n",
    "- We have to include all the basic characters (otherwise we won’t be able to tokenize every word).\n",
    "- For the bigger substrings, we can use the most frequent substrings in the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bc11ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('in', 6437), ('th', 6241), ('he', 4833), ('er', 4585), ('re', 4475), ('an', 4311), ('the', 3977), ('on', 3842), ('es', 3360), ('ar', 3269)]\n"
     ]
    }
   ],
   "source": [
    "character_freqs = defaultdict(int)\n",
    "subwords_freqs = defaultdict(int)\n",
    "for word, freq in word_freqs.items():\n",
    "    for i in range(len(word)):\n",
    "        character_freqs[word[i]] += freq\n",
    "        # Loop through the subwords of length at least 2\n",
    "        for j in range(i + 2, len(word) + 1):\n",
    "            subwords_freqs[word[i:j]] += freq\n",
    "\n",
    "# Sort subwords by frequency\n",
    "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "print(sorted_subwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c7558f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60023"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_subwords)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90ed43b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "We group the characters with the best subwords to arrive at an initial vocabulary of size 2000:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e388d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_freqs = (\n",
    "    list(character_freqs.items()) + sorted_subwords[: 2000 - len(character_freqs)]\n",
    ")\n",
    "token_freqs = {token: freq for token, freq in token_freqs}\n",
    "len(token_freqs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8093f3c",
   "metadata": {},
   "source": [
    "Next, we compute the sum of all frequencies, to convert the frequencies into probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dac4d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd04301d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The main function is the one that tokenizes words using the Viterbi algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0e97350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word, model):\n",
    "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
    "        {\"start\": None, \"score\": None} for _ in range(len(word))\n",
    "    ]\n",
    "    for start_idx in range(len(word)):\n",
    "        # This should be properly filled by the previous steps of the loop\n",
    "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "            token = word[start_idx:end_idx]\n",
    "            if token in model and best_score_at_start is not None:\n",
    "                score = model[token] + best_score_at_start\n",
    "                # If we have found a better segmentation ending at end_idx, we update\n",
    "                if (\n",
    "                    best_segmentations[end_idx][\"score\"] is None\n",
    "                    or best_segmentations[end_idx][\"score\"] > score\n",
    "                ):\n",
    "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "\n",
    "    segmentation = best_segmentations[-1]\n",
    "    if segmentation[\"score\"] is None:\n",
    "        # We did not find a tokenization of the word -> unknown\n",
    "        return [\"<unk>\"], None\n",
    "\n",
    "    score = segmentation[\"score\"]\n",
    "    start = segmentation[\"start\"]\n",
    "    end = len(word)\n",
    "    tokens = []\n",
    "    while start != 0:\n",
    "        tokens.insert(0, word[start:end])\n",
    "        next_start = best_segmentations[start][\"start\"]\n",
    "        end = start\n",
    "        start = next_start\n",
    "    tokens.insert(0, word[start:end])\n",
    "    return tokens, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "056f330a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['app', 'le'], 16.199784937807312)\n",
      "(['investment'], 9.955290111180942)\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"apple\", model))\n",
    "print(encode_word(\"investment\", model))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41aefe0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Compute the loss:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10c70cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "802891.4150846584"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_loss(model):\n",
    "    loss = 0\n",
    "    for word, freq in word_freqs.items():\n",
    "        _, word_loss = encode_word(word, model)\n",
    "        loss += freq * word_loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "compute_loss(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53ac4e1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Computing the scores for each token:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56a5975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def compute_scores(model):\n",
    "    scores = {}\n",
    "    model_loss = compute_loss(model)\n",
    "    for token, score in model.items():\n",
    "        # We always keep tokens of length 1\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        model_without_token = copy.deepcopy(model)\n",
    "        _ = model_without_token.pop(token)\n",
    "        scores[token] = compute_loss(model_without_token) - model_loss\n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = compute_scores(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cc44ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.53267826826777\n",
      "239.44326648849528\n",
      "326.00756032345816\n",
      "113.19295595935546\n",
      "435.93838484131265\n"
     ]
    }
   ],
   "source": [
    "print(scores[\"app\"])\n",
    "print(scores[\"le\"])\n",
    "print(scores[\"investment\"])\n",
    "print(scores[\"invest\"])\n",
    "print(scores[\"ment\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d36b1a55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Iterate until we have the desired vocabulary size:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc7eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_to_remove = 0.1\n",
    "while len(model) > 1000:\n",
    "    scores = compute_scores(model)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
    "    # Remove percent_to_remove tokens with the lowest scores.\n",
    "    for i in range(int(len(model) * percent_to_remove)):\n",
    "        _ = token_freqs.pop(sorted_scores[i][0])\n",
    "\n",
    "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "053846b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "To tokenize a sentence, we can apply this function to each word:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d001097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, model):\n",
    "    words = pre_tokenize(text)\n",
    "    encoded_words = [encode_word(word, model)[0] for word in words]\n",
    "    return sum(encoded_words, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4cdb5b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['investment', 'o', 'pport', 'un', 'ities', 'in', 'the', 'company']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenize(\"investment opportunities in the company\", model)\n",
    "print(tokenized_text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "- [NLP Tokenization](https://medium.com/nerd-for-tech/nlp-tokenization-2fdec7536d17)\n",
    "- [Hugging Face Tokenizers](https://huggingface.co/course/chapter6/1?fw=pt)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e60fe9bf",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe900edd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
