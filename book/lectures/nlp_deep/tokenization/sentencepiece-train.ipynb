{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2c4ac1b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# How to train SentencePiece\n",
    "\n",
    "- Assume that we have a large collection of bigrams, greater than what we ultimately want to use in our model.\n",
    "- To train a SentencePiece model, we want to maximize the probability of obtaining a particular tokenization $X = (x_1, x_2, \\cdots, x_n)$ of the corpus, given the unigram probabilities $p(x_i), p(x_2), \\cdots, p(x_n)$.\n",
    "- The actual tokenization $X$ is not observed, we only observe the un-tokenized sequence $X$.\n",
    "- This is a classic problem of maximum likelihood estimation, and we can solve it by the EM algorithm.\n",
    "- The problem is that thte $x_i$ are all of different lengths, and we cannot apply the EM algorithm directly.\n",
    "- Instead, we should use a Bayesian approach to solve this problem.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7005fe57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The SentencePiece training objective is given by the following equation:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = - \\sum_{s=1}^{|D|} \\log P(X^{(s)}) = - \\sum_{s=1}^{|D|} \\log (\\sum_{\\mathbf{x} \\in S(\\mathbf{x})} P(\\mathbf{x}) )\n",
    "$$\n",
    "\n",
    "where the $\\mathbf{x}$ is a unigram sequence, and $S(\\mathbf{x})$ is the set of all possible sequences that can be generated from $\\mathbf{x}$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad63ab68",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "The steps are as follows:\n",
    "\n",
    "1. Initialize the unigram probabilities $p(x_i)$. The frequency of each unigram is used as the initial value.\n",
    "2. M-step: Compute the most likely sequence $\\mathbf{x}$ given the current unigram probabilities $p(x_i)$.\n",
    "3. E-step: Given the current most likely sequence $\\mathbf{x}$, update the unigram probabilities $p(x_i)$. In Bayesian setting, the unigram probabilities are defined as:\n",
    "\n",
    "   $$\n",
    "   p(x_i | \\mathbf{x}) = \\frac{c_i}{\\sum_{j=1}^{|V|} c_j} \\implies \\frac{e^{\\psi(c_i)}}{\\sum_{j=1}^{|V|} e^{\\psi(c_j)}} = \\frac{e^{\\psi(c_i)}}{e^{\\psi(\\sum_{j=1}^{|V|} c_j)}} = \\frac{e^{\\psi(c_i)}}{e^{\\psi(c_1) + \\cdots + \\psi(c_n)}}\n",
    "   $$\n",
    "\n",
    "   where $c_i$ is the frequency of the unigram $x_i$ in the corpus, $|V|$ is the size of the vocabulary, and $\\psi(c_i)$ is the digamma function.\n",
    "\n",
    "4. Repeat the M-step and E-step until the unigram probabilities converge. The log-likelihood is monotonically increasing, so we can stop the training when the log-likelihood does not increase for a certain number of iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c310d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ekorpkit import eKonf\n",
    "from ekorpkit.tokenizers.trainers.unigram import UnigramTrainer\n",
    "\n",
    "sp = UnigramTrainer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d287b2ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Initialize the unigram probabilities $p(x_i)$. The frequency of each unigram is used as the initial value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf8cce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def initialize_vocab(self, words):\n",
      "        word_freqs = self.get_word_freqs(words)\n",
      "        sorted_subwords, characters = self.initialize_subwords(word_freqs)\n",
      "\n",
      "        alphabet = {char: 0 for char in self.initial_alphabet if char not in characters}\n",
      "        tokens = list(characters.items()) + sorted_subwords + list(alphabet.items())\n",
      "        tokens = {token: freq for token, freq in tokens}\n",
      "        tokens = collections.Counter(tokens)\n",
      "        return tokens, characters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eKonf.viewsource(sp.initialize_vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75582211",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- M-step: Compute the most likely sequence $\\mathbf{x}$ given the current unigram probabilities $p(x_i)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd8238c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def M_step(self, text, trie):\n",
      "        loss, p = self.forward_step(text, trie)\n",
      "        tokenization = self.backward_step(text, p)\n",
      "        return tokenization, loss\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eKonf.viewsource(sp.M_step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb46c7f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- E-step: Given the current most likely sequence $\\mathbf{x}$, update the unigram probabilities $p(x_i)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f7d7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def E_step(self, tokenization, trie):\n",
      "        # get the new token counts based on updated tokenization\n",
      "        counts = collections.Counter(tokenization)\n",
      "        norm = sum(list(counts.values()))\n",
      "\n",
      "        # we are returning the log probabilties here (alpha=0 prior)\n",
      "        logsum = digamma(norm)\n",
      "        for k, v in counts.items():\n",
      "            counts[k] = digamma(v) - logsum\n",
      "\n",
      "        for k, v in counts.items():\n",
      "            trie.set_value(k, v)\n",
      "        return trie\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eKonf.viewsource(sp.E_step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b525684",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Repeat the M-step and E-step until the unigram probabilities converge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aea9adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def EM_round(self, text, tokens, delta=0.01, n_iterations=10):\n",
      "        tokenization, old_loss = self.M_step(text, self.trie)\n",
      "        for step in range(n_iterations):\n",
      "            print(f\"EM iter {step}: \", end=\"\")\n",
      "            loss, tokenization, trie = self.EM_step(text, tokenization, self.trie)\n",
      "            print(f\"Loss={loss:.2f}\")\n",
      "            if abs(old_loss - loss) < delta:\n",
      "                break\n",
      "            old_loss = loss\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eKonf.viewsource(sp.EM_round)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f03d760",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Finding the optimal segmentation\n",
    "\n",
    "- If all of the subwords were of the same length, we could apply the Viterbi algorithm to find the optimal segmentation.\n",
    "- The Viterbi algorithm is a dynamic programming algorithm that finds the most likely sequence of states, given a sequence of observations and a model of the transition probabilities between states and the emission probabilities of the observations given the states.\n",
    "  - You have some hidden states $z_1, z_2, \\cdots, z_n$, and you want to transition from $z_1 \\rightarrow z_2 \\rightarrow \\cdots \\rightarrow z_n$, and you know the transition matrix $A_{ij}$, giving the probability of transitioning from $z_i^{(1)}$ to $z_j^{(2)}$, where $i$ and $j$ are the hidden states, and the superscript indicates the sequence order.\n",
    "- The problem is that $A$ is not between adjacent states.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "599ab185",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- Consider tokenizing the word `hello` given the subwords {`he`, `h`, `ll`, `e`, `o`, `hell`}.\n",
    "- We can generate the following figure:\n",
    "\n",
    "  ![](figs/transition.png)\n",
    "\n",
    "  - Each arrow represents a transition, and the weight of the arrow is the probability of the transition.\n",
    "  - The goal is to pick arrows that we arrive at `<eos>` (end of sequence) with the highest probability.\n",
    "\n",
    "- This problem has optimal substructure, so we can apply dynamic programming to solve it.\n",
    "- For example, assume that we are at the state (4).\n",
    "\n",
    "  - There are three arrows that can lead to the state (4), a red, a blue, and a green arrow.\n",
    "  - The highest probability at the state (4) is just the best path from the previous state, plus the probability of the transition.\n",
    "\n",
    "    $$\n",
    "    p_i = \\max_{j \\le i} (p_j p_{j \\rightarrow i})\n",
    "    $$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ce3d585",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "- The Trie structure is used to find the optimal segmentation.\n",
    "\n",
    "  - The Trie structure is a tree structure that is used to store a set of strings.\n",
    "  - The following figure shows the Trie structure for the subwords {`h`, `he`, `hell`, `hello`}:\n",
    "\n",
    "    ![](figs/trie.png)\n",
    "\n",
    "  - The root node is the start of the sequence, `<sos>`.\n",
    "  - Any time we encounter an `end` node, it means that everything in the path from `<sos>` to `end` is a valid subword.\n",
    "  - The root node `<sos>` will begin with exactly one branch for every unique first character in the vocabulary.\n",
    "  - As we grow the available subwords, we create more branches.\n",
    "  - The Trie is going to be the fundamental data structure that the tokenizer uses to store and retrieve the subwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f13e3b76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def initialize_trie(self, tokens):\n",
      "        trie = Trie()\n",
      "        norm = sum(list(tokens.values()))\n",
      "        logsum = digamma(norm)\n",
      "\n",
      "        maxlen = 0\n",
      "        for tok, val in tokens.items():\n",
      "            trie.add(tok, digamma(val) - logsum)\n",
      "            maxlen = max(maxlen, len(tok))\n",
      "\n",
      "        return trie, maxlen\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eKonf.viewsource(sp.initialize_trie)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6011d611",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Fitting the model\n",
    "\n",
    "- One of the algorithm for finding the optimal sequence from the Trie is a forwards-backwards algorithm.\n",
    "  - It is a special subset of the sum-product algorithm for training directed graphical models.\n",
    "  - More sophisticated algorithms include the Forward-DP Backward-A\\* algorithm, and the Forward-Filtering and Backward-Sampling algorithm (FFBS).\n",
    "- When we compute the forward step, we also store the length of the longest subword that ends at the current position.\n",
    "- This allows us to backtrack from the end of the sequence to the beginning, and find the optimal segmentation, since the length of the arrow is the length of the subword.\n",
    "- The EM step puts together the E step and the M step, where the E step is updating the Trie, and the M step is finding the optimal segmentation using the forwards-backwards algorithm explained above.\n",
    "- Then, fitting the model is just a matter of repeating the EM step until the log-likelihood converges.\n",
    "- One more thing to consider is to get the desired vocabulary size by pruning the vocabulary.\n",
    "  - First, prepare more subword tokens than the desired vocabulary size.\n",
    "  - After each EM step, remove the least probable, say 10%, of the subwords.\n",
    "  - Repeat this process until the desired vocabulary size is reached.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65c232e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def forward_step(self, text, trie):\n",
      "        N = len(text)\n",
      "\n",
      "        # d[i] contains the maximum log_prob of any tokenization\n",
      "        # of text[:i], initialized to 0 (i.e. log(0)=-infty)\n",
      "        d = [-np.inf] * (N + 1)\n",
      "\n",
      "        # p[i] (stands for parent) contains the number of characters of\n",
      "        # the final token in the most likely sequence that ends at index i\n",
      "        p = [None] * (N + 1)\n",
      "        d[0] = 0\n",
      "\n",
      "        for i in range(1, N + 1):\n",
      "\n",
      "            # find all possible final words. Have to look back\n",
      "            # a distance set by the length of the longest token\n",
      "            for j in range(max(i - self.maxlen, 0), i):\n",
      "\n",
      "                final_token = text[j:i]\n",
      "                final_value = trie.get_value(final_token)\n",
      "\n",
      "                # if the current ending word has a higher log-probability,\n",
      "                # save that value and store the word (i.e. # chars to backtrack)\n",
      "                if final_value and d[j] + final_value > d[i]:\n",
      "                    d[i] = d[j] + final_value\n",
      "                    p[i] = len(final_token)\n",
      "            if p[i] is None:\n",
      "                raise ValueError(f\"Encountered unknown token '{text[i-1]}'.\")\n",
      "\n",
      "        loss = d[-1]\n",
      "        return loss, p\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eKonf.viewsource(sp.forward_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d14ac3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def backward_step(self, text, p):\n",
      "        idx = len(p)\n",
      "        tokenization = []\n",
      "        while idx > 1:\n",
      "            # move back the number of steps p tells you to\n",
      "            next_idx = idx - p[idx - 1]\n",
      "\n",
      "            # extract the final token\n",
      "            tok = text[next_idx - 1 : idx - 1]\n",
      "            tokenization.append(tok)\n",
      "\n",
      "            idx = next_idx\n",
      "        tokenization = list(reversed(tokenization))\n",
      "        return tokenization\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eKonf.viewsource(sp.backward_step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30673c24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Subword sampling\n",
    "\n",
    "- To find alternative segmentations, we can save the n-best paths in the forward-backward step.\n",
    "- Now, we can sample from the n-best paths to find alternative segmentations.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "904cf20e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c32d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekorpkit import eKonf\n",
    "\n",
    "cfg = eKonf.compose(\"path\")\n",
    "cfg.cache.uri = \"https://github.com/entelecheia/ekorpkit-book/raw/main/assets/data/us_equities_news_sampled.zip\"\n",
    "data = eKonf.load_data(\"us_equities_news_sampled.parquet\", cfg.cached_path)\n",
    "texts = data.text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8453d743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Round 1. Vocab size: 9641 ---\n",
      "EM iter 0: Loss=-596166.15\n",
      "EM iter 1: Loss=-593240.93\n",
      "EM iter 2: Loss=-592453.62\n",
      "EM iter 3: Loss=-592143.08\n",
      "EM iter 4: Loss=-592028.75\n",
      "--- Round 2. Vocab size: 7231 ---\n",
      "EM iter 0: Loss=-616021.42\n",
      "EM iter 1: Loss=-615565.28\n",
      "EM iter 2: Loss=-615379.74\n",
      "EM iter 3: Loss=-615290.66\n",
      "EM iter 4: Loss=-615283.06\n",
      "--- Round 3. Vocab size: 5424 ---\n",
      "EM iter 0: Loss=-640810.86\n",
      "EM iter 1: Loss=-640411.09\n",
      "EM iter 2: Loss=-640241.08\n",
      "EM iter 3: Loss=-640186.43\n",
      "EM iter 4: Loss=-640155.71\n"
     ]
    }
   ],
   "source": [
    "from ekorpkit.tokenizers.sentencepiece import SentencePieceUnigramTokenizer\n",
    "\n",
    "texts = data.text[:100]\n",
    "vocab_size = 5000\n",
    "\n",
    "sp = SentencePieceUnigramTokenizer()\n",
    "sp.train_from_iterator(\n",
    "    texts, vocab_size=vocab_size, min_frequency=10, initial_alphabet=[\".\", \"!\", \"?\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993053aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/tokenizers/sp_unigram/vocab.json',\n",
       " '../data/tokenizers/sp_unigram/config.json']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.save(\"../data/tokenizers\", \"sp_unigram\", pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77d77b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekorpkit.tokenizers.sentencepiece import SentencePieceUnigramTokenizer\n",
    "\n",
    "sp = SentencePieceUnigramTokenizer.load(\"../data/tokenizers\", \"sp_unigram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc92b7a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "investment opportunities in the company.\n",
      "['▁Investment', '▁opportunities', '▁in', '▁the', '▁company.']\n",
      "[None, [1], [2], [3], [4], [5], [6], [7], [8], [1]]\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Investment opportunities in the company.\"\n",
    "print(sp._tokenizer.normalize(sequence))\n",
    "print(sp._tokenizer.pre_tokenize(sequence))\n",
    "print(sp._tokenizer.generalized_forward_step(\"▁company.\", sp._tokenizer.trie, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ccb2092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['▁', 'investment', '▁opportun', 'iti', 'es', '▁i', 'n', '▁', 'th', 'e', '▁co', 'mpany', '.']\n",
      "1 ['▁', 'in', 'vestment', '▁opportuni', 'ties', '▁', 'in', '▁', 'th', 'e', '▁', 'company', '.']\n",
      "2 ['▁', 'investment', '▁o', 'pportuni', 't', 'ies', '▁', 'i', 'n', '▁the', '▁', 'company', '.']\n",
      "3 ['▁investment', '▁', 'op', 'port', 'un', 'it', 'i', 'es', '▁', 'i', 'n', '▁the', '▁', 'c', 'o', 'mpany', '.']\n",
      "4 ['▁', 'investmen', 't', '▁', 'opport', 'un', 'itie', 's', '▁in', '▁', 'th', 'e', '▁', 'compan', 'y', '.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    tokenized_text = sp.tokenize(sequence, nbest_size=5)\n",
    "    print(i, tokenized_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66d50cc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "- [SentencePiece](https://github.com/google/sentencepiece)\n",
    "- [SentencePiece Tokenizer Demystified](https://towardsdatascience.com/sentencepiece-tokenizer-demystified-d0a3aac19b15)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
