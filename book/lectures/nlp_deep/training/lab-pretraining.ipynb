{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Pretraining Language Models\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset and tokenizer, in this lab, we will train a language model on a large corpus of text from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e221b829f9574fbf988c75775e1603e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 70.7 ms (started: 2022-11-21 00:59:58 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user id 'entelecheia' will be used during this lab\n",
      "time: 893 ms (started: 2022-11-21 01:06:46 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "token = HfFolder.get_token()\n",
    "if token is None:\n",
    "    token = os.environ[\"HF_USER_ACCESS_TOKEN\"]\n",
    "\n",
    "if token is None:\n",
    "    raise ValueError(\"Please login to huggingface_hub\")\n",
    "\n",
    "user_id = HfApi().whoami(token)[\"name\"]\n",
    "\n",
    "print(f\"user id '{user_id}' will be used during this lab\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Pretraining\n",
    "\n",
    "In this lab, we will train a BERT-like model using masked-language modeling, one of the two pretraining tasks used in the original BERT paper.\n",
    "\n",
    "### What is BERT?\n",
    "\n",
    "BERT is a large-scale language model that was trained on the English Wikipedia using a masked-language modeling objective. The model was then fine-tuned on a variety of downstream tasks, including question answering, natural language inference, and sentiment analysis. BERT was the first large-scale language model to be pre-trained using a deep bidirectional architecture and outperformed previous language models on a variety of tasks.\n",
    "\n",
    "BERT was originally pre-trained on 1 Million Steps with a global batch size of 256.\n",
    "\n",
    "> \"We train with batch size of 256 sequences (256 sequences \\* 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40 epochs over the 3.3 billion word corpus.\"\n",
    "\n",
    "For more information, see the lecture notes on BERT.\n",
    "\n",
    "### Masked-Language Modeling (MLM)\n",
    "\n",
    "Masked-language modeling is a pretraining task where we mask some of the input tokens and train the model to predict the original value of the masked tokens. For example, if we have the sentence \"The dog ate the apple\", we can mask the word \"ate\" and train the model to predict the original value of the masked token. The model will then learn to predict the original value of the masked tokens based on the context of the sentence.\n",
    "\n",
    "Example:\n",
    "\n",
    "> Input: \"The dog [MASK] the apple\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Dataset\n",
    "\n",
    "Before training our language model, we need to preprocess our dataset. We will use our tokenizer to tokenize our dataset and then convert the tokens to their IDs. If we have a sentence that is longer than the maximum sequence length, we will truncate the sentence. If the sentence is shorter than the maximum sequence length, we will pad the sentence with the padding token.\n",
    "\n",
    "Unlike the original BERT paper, we will not use the WordPiece tokenization algorithm. Instead, we will use the `unigram` tokenization algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30000\n",
      "is_fast: True\n",
      "Vocab size: 30000\n",
      "{'input_ids': [1, 8, 14690, 10, 8, 968, 8, 6871, 8, 42, 8, 2777, 72, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/content/drive/MyDrive/workspace/projects/ekorpkit-book/tokenizers/enko_wiki/tokenizer_config.json',\n",
       " '/content/drive/MyDrive/workspace/projects/ekorpkit-book/tokenizers/enko_wiki/special_tokens_map.json',\n",
       " '/content/drive/MyDrive/workspace/projects/ekorpkit-book/tokenizers/enko_wiki/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 67 ms (started: 2022-11-19 10:55:20 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer_path = \"tokenizers/enko_wiki/enko_wiki_unigram_huggingface_vocab_30000.json\"\n",
    "context_length = 512\n",
    "\n",
    "unigram_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "print(f\"Vocab size: {unigram_tokenizer.get_vocab_size()}\")\n",
    "unigram_tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", unigram_tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", unigram_tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=unigram_tokenizer,\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_length=True,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    padding_side=\"right\",\n",
    ")\n",
    "\n",
    "print(f\"is_fast: {tokenizer.is_fast}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "print(tokenizer(\"Hello, my dog is cute\"))\n",
    "tokenizer.save_pretrained(project_dir + \"/tokenizers/enko_wiki\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_fast: True\n",
      "time: 59.6 ms (started: 2022-11-21 01:03:21 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/tokenizers/enko_wiki\")\n",
    "print(f\"is_fast: {tokenizer.is_fast}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 8, 14690, 10, 8, 235, 8, 202, 8, 15219, 489, 2, 8, 37, 8, 235, 8, 15219, 8, 11241, 8, 80, 8, 65, 9, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.2 ms (started: 2022-11-21 01:07:07 +00:00)\n"
     ]
    }
   ],
   "source": [
    "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eeddc62f7db4f55a039916fbd57d972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-f34802e795f4ed05\n",
      "WARNING:datasets.builder:Reusing dataset text (/workspace/data/tbts/.cache/huggingface/datasets/text/default-f34802e795f4ed05/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 3618972\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.02 s (started: 2022-11-19 09:39:39 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_dir =  \"data/tokenizers/enko_filtered_chunk\"\n",
    "\n",
    "dataset = load_dataset(\"text\", data_dir=data_dir, split=\"train\")\n",
    "dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15.3 ms (started: 2022-11-19 09:55:12 +00:00)\n"
     ]
    }
   ],
   "source": [
    "text_column = \"text\"\n",
    "\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[text_column],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proc = 20\n",
    "\n",
    "# preprocess dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize, batched=True, remove_columns=[text_column], num_proc=num_proc\n",
    ")\n",
    "tokenized_dataset.features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= context_length:\n",
    "        total_length = (total_length // context_length) * context_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + context_length] for i in range(0, total_length, context_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(group_texts, batched=True, num_proc=num_proc)\n",
    "\n",
    "# shuffle dataset\n",
    "tokenized_dataset = tokenized_dataset.shuffle(seed=1234)\n",
    "\n",
    "print(f\"the dataset contains in total {len(tokenized_dataset)*context_length} tokens\")\n",
    "# the dataset contains in total 137,816,832 tokens\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 137,816,832 tokens in our dataset. For reference, the original BERT paper used 3.2 billion tokens, and GPT-3 uses 300 billion tokens.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a New Model\n",
    "\n",
    "We will initialize a new model using the `bert-base-uncased` configuration. We will then save the configuration to a file so that we can use it later when we load the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.4 s (started: 2022-11-19 11:29:40 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM\n",
    "\n",
    "tk_path = \"tokenizers/enko_wiki\"\n",
    "\n",
    "# Load codeparrot tokenizer trained for Python code tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(tk_path)\n",
    "\n",
    "# Configuration\n",
    "config_kwargs = {\n",
    "    \"vocab_size\": len(tokenizer),\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"mask_token_id\": tokenizer.mask_token_id,\n",
    "    \"cls_token_id\": tokenizer.cls_token_id,\n",
    "    \"sep_token_id\": tokenizer.sep_token_id,\n",
    "    \"unk_token_id\": tokenizer.unk_token_id,\n",
    "}\n",
    "\n",
    "# # Load model with config and push to hub\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\", **config_kwargs)\n",
    "model = AutoModelForMaskedLM.from_config(config)\n",
    "\n",
    "model_path = \"models/enko_wiki_bert_base_uncased\"\n",
    "model.save_pretrained(model_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model have 109.1 million parameters just like the original BERT model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT size: 109.1M parameters\n",
      "time: 2.07 s (started: 2022-11-19 11:23:15 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "\n",
    "model = BertForMaskedLM(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"BERT size: {model_size/1000**2:.1f}M parameters\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a DataCollator\n",
    "\n",
    "Before we can start training, we need to set up a data collator that will be used to collate the batches of data. We will use the `DataCollatorForLanguageModeling` data collator, which will take care of masking the tokens and padding the sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 01:24:31.069901: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.59 s (started: 2022-11-21 01:24:30 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Tokenized Dataset\n",
    "\n",
    "Our dataset is already tokenized, there are 268,366 examples in our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 268366\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 269 ms (started: 2022-11-21 01:24:33 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset_dir = \"data/tokenized_datasets/enko_filtered\"\n",
    "\n",
    "tokenized_dataset = Dataset.load_from_disk(dataset_dir)\n",
    "tokenized_dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the output of the first batch of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 512])\n",
      "token_type_ids shape: torch.Size([5, 512])\n",
      "attention_mask shape: torch.Size([5, 512])\n",
      "labels shape: torch.Size([5, 512])\n",
      "time: 70 ms (started: 2022-11-21 01:24:35 +00:00)\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_dataset[i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "We will configure the training arguments and then set up a trainer to train our model.\n",
    "\n",
    "### Configure the Training Arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.43 s (started: 2022-11-19 11:34:48 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "time: 47.9 ms (started: 2022-11-19 11:34:51 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "acc_state = {str(k): str(v) for k, v in accelerator.state.__dict__.items()}\n",
    "device = accelerator.device\n",
    "\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "trainer = accelerator.prepare(trainer)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(model_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took 6h 33m 0.0s to train our model for 40 epochs.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "\n",
    "We will load our model and test it on a few examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:652] 2022-11-22 19:14:02,688 >> loading configuration file /content/drive/MyDrive/workspace/projects/ekorpkit-book/models/enko_wiki_bert_base_uncased/config.json\n",
      "[INFO|configuration_utils.py:706] 2022-11-22 19:14:02,690 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"/content/drive/MyDrive/workspace/projects/ekorpkit-book/models/enko_wiki_bert_base_uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 4,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 6,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2155] 2022-11-22 19:14:02,691 >> loading weights file /content/drive/MyDrive/workspace/projects/ekorpkit-book/models/enko_wiki_bert_base_uncased/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2608] 2022-11-22 19:14:04,867 >> All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:2616] 2022-11-22 19:14:04,873 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at /content/drive/MyDrive/workspace/projects/ekorpkit-book/models/enko_wiki_bert_base_uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 19:14:04,888 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 19:14:04,888 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 19:14:04,889 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-11-22 19:14:04,889 >> loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.3 s (started: 2022-11-22 19:14:02 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_path = \"models/enko_wiki_bert_base_uncased\"\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.1881457269191742, 'token': 1183, 'token_str': '만든', 'sequence': '▁ 처음으로 ▁ 대중적 으로 ▁ 만든 ▁ 롤플레잉 ▁ 게임 이다.'}\n",
      "{'score': 0.14176464080810547, 'token': 3567, 'token_str': '제작한', 'sequence': '▁ 처음으로 ▁ 대중적 으로 ▁ 제작한 ▁ 롤플레잉 ▁ 게임 이다.'}\n",
      "{'score': 0.04182405769824982, 'token': 15022, 'token_str': '롤플레잉', 'sequence': '▁ 처음으로 ▁ 대중적 으로 ▁ 롤플레잉 ▁ 롤플레잉 ▁ 게임 이다.'}\n",
      "{'score': 0.035137832164764404, 'token': 3225, 'token_str': '개발한', 'sequence': '▁ 처음으로 ▁ 대중적 으로 ▁ 개발한 ▁ 롤플레잉 ▁ 게임 이다.'}\n",
      "{'score': 0.033710286021232605, 'token': 3171, 'token_str': '아시안', 'sequence': '▁ 처음으로 ▁ 대중적 으로 ▁ 아시안 ▁ 롤플레잉 ▁ 게임 이다.'}\n",
      "time: 223 ms (started: 2022-11-22 19:14:05 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# perform predictions\n",
    "example = \"처음으로 대중적으로 <mask> 롤플레잉 게임이다.\"\n",
    "for prediction in fill_mask(example):\n",
    "    print(prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
