{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1aec77a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "### Input:\n",
    "\n",
    "- A set of documents (e.g. text files), $D$\n",
    "\n",
    "### Output (tokens):\n",
    "\n",
    "- A sequence, $W$ , containing a list of tokens – words or word pieces for use in natural language processing\n",
    "\n",
    "### Output (n-grams):\n",
    "\n",
    "- A matrix, $X$, containing statistics about word/phrase frequencies in those documents.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afda69a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Tokens\n",
    "\n",
    "The most basic unit of representation in a text.\n",
    "\n",
    "- characters: documents as sequence of individual letters {h,e,l,l,o, ,w,o,r,l,d}\n",
    "- words: split on white space {hello, world}\n",
    "- n-grams: learn a vocabulary of phrases and tokenize those:\n",
    "  > `“hellow world → hellow_world”`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "815632b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Goals of Tokenization\n",
    "\n",
    "To summarize: A major goal of tokenization is to produce features that are\n",
    "\n",
    "- `predictive` in the learning task\n",
    "- `interpretable` by human investigators\n",
    "- `tractable` enough to be easy to work with\n",
    "\n",
    "Two broad approaches:\n",
    "\n",
    "1. convert documents to vectors, usually frequency distributions over pre-processed n-grams.\n",
    "2. convert documents to sequences of tokens, for inputs to sequential models.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87e72ddb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### A Traditional Tokenization Pipeline\n",
    "\n",
    "- Extract text from documents (e.g. PDF, HTML, XML, …)\n",
    "- Tokenize text into words\n",
    "- Normalize words (e.g. lowercasing, stemming, lemmatization)\n",
    "- Remove stop words (e.g. “the”, “a”, “an”, “in”, …)\n",
    "- Build a vocabulary of words\n",
    "- Convert documents to vectors of word counts or TF-IDF scores\n",
    "- Train a model on those vectors\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22066f8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Subword Tokenization for Sequence Models\n",
    "\n",
    "Modern transformer models (e.g. BERT, GPT) use subword tokenization:\n",
    "\n",
    "- construct character-level n-grams\n",
    "- whitespace treated the same as letters\n",
    "- all letters to lowercase, but add a special character for the next letter being capitalized.\n",
    "\n",
    "e.g., BERT’s WordPiece tokenizer:\n",
    "\n",
    "- character-level byte-pair encoder\n",
    "- learns character n-grams to breaks words like “playing” into “play” and “##ing”.\n",
    "- have to fix a vocabulary size: e.g. BERT uses 30K.\n",
    "\n",
    "![](figs/3.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af8e7672",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Segmenting paragraphs/sentences\n",
    "\n",
    "Many tasks should be done on sentences, rather than corpora as a whole.\n",
    "\n",
    "- spaCy is a good (but not perfect) job of splitting sentences, while accounting for periods on abbreviations, etc.\n",
    "- pySBD is a better option for splitting sentences.\n",
    "\n",
    "There isn’t a grammar-based paragraph tokenizer.\n",
    "\n",
    "- most corpora have new paragraphs annotated.\n",
    "- or use line breaks.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbd67c05",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Pre-processing\n",
    "\n",
    "An important piece of the “art” of text analysis is deciding what data to throw out.\n",
    "\n",
    "- Uninformative data add noise and reduce statistical precision.\n",
    "- They are also computationally costly.\n",
    "\n",
    "Pre-processing choices can affect down-stream results, especially in unsupervised learning tasks (Denny and Spirling 2017).\n",
    "\n",
    "- some features are more interpretable: “govenor has” / “has discretion” vs “govenor has discretion”.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89774b97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Capitalization\n",
    "\n",
    "Removing capitalization is a standard corpus normalization technique\n",
    "\n",
    "- usually the capitalized/non-capitalized version of a word are equivalent – e.g. words showing up capitalized at beginning of sentence\n",
    "- → capitalization not informative.\n",
    "\n",
    "Also: what about “the first amendment” versus “the First Amendment”?\n",
    "\n",
    "- Compromise: include capitalized version of words not at beginning of sentence.\n",
    "\n",
    "For some tasks, capitalization is important\n",
    "\n",
    "- needed for sentence splitting, part-of-speech tagging, syntactic parsing, and semantic role labeling.\n",
    "- For sequence data, e.g. language modeling – huggingface tokenizer takes out capitalization but then add a special “capitalized” token before the word.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "386fcb60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Punctuation\n",
    "\n",
    "![](figs/4.png)\n",
    "(Source: Chris Bail text data slides.)\n",
    "\n",
    "Inclusion of punctuation depends on your task:\n",
    "\n",
    "- if you are vectorizing the document as a bag of words or bag of n-grams, punctuation won’t be needed.\n",
    "- like capitalization, punctuation is needed for annotations (sentence splitting, parts of speech, syntax, roles, etc)\n",
    "  - also needed for language models.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb0f16c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Numbers\n",
    "\n",
    "for classification using bag of words:\n",
    "\n",
    "- can drop numbers, or replace with special characters\n",
    "\n",
    "for language models:\n",
    "\n",
    "- just treat them like letters.\n",
    "- GPT-3 can solve math problems (but not well, this is an area of research)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "383593b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Drop Stopwords?\n",
    "\n",
    "![](figs/5.png)\n",
    "\n",
    "- Stopwords are words that are so common that they don’t carry much information.\n",
    "- can drop stopwords by themselves, but keep them as part of phrases.\n",
    "- can filter out words and phrases using part-of-speech tags (later).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5665c1c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Stemming/lemmatizing\n",
    "\n",
    "- Effective dimension reduction with little loss of information.\n",
    "- Lemmatizer produces real words, but N-grams won’t make grammatical sense\n",
    "  - e.g., “I am running” → “I am run”\n",
    "\n",
    "![](figs/6.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa22f957",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## What are N-grams?\n",
    "\n",
    "- N-grams are contiguous sequences of n tokens.\n",
    "  - Bigrams: 2-grams\n",
    "  - Trigrams: 3-grams\n",
    "  - Quadgrams: 4-grams\n",
    "- Google Developers recommend tf-idf-weighted bigrams as a baseline for text classification.\n",
    "\n",
    "For example, the sentence “The quick brown fox jumps over the lazy dog” has the following bigrams:\n",
    "\n",
    "> [“The quick”, “quick brown”, “brown fox”, “fox jumps”, “jumps over”, “over the”, “the lazy”, “lazy dog”]\n",
    "\n",
    "for trigrams:\n",
    "\n",
    "> [“The quick brown”, “quick brown fox”, “brown fox jumps”, “fox jumps over”, “jumps over the”, “over the lazy”, “the lazy dog”]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3410c3c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Text classification flowchart (from Google Developers):**\n",
    "\n",
    "![](figs/10.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7455508",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### N-grams and high dimensionality\n",
    "\n",
    "- N-grams will blow up your feature space:\n",
    "  - 1-grams: 1000 words → 1000 features\n",
    "  - 2-grams: 1000 words → 500,500 features\n",
    "- Filtering out low-frequency and uninformative n-grams is important.\n",
    "- Google Developers say that a feature space of 20,000 features will work well for descriptive and predictive text classification.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ad13d6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Hashing Vectorizer\n",
    "\n",
    "Rather than make a one-to-one lookup for each n-gram, put n-grams through a hashing function that takes an arbitrary string and outputs an integer in some range (e.g. 1 to 10,000).\n",
    "\n",
    "- This is a lossy transformation, but it can be useful for very large feature spaces.\n",
    "- The hashing function is deterministic, so the same string will always map to the same integer.\n",
    "\n",
    "![](figs/11.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "270ed31c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2257, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vectorizer = HashingVectorizer(\n",
    "    n_features=2**4, stop_words=\"english\", alternate_sign=False\n",
    ")\n",
    "\n",
    "X = vectorizer.transform(twenty_train.data)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9de0380c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Collocations\n",
    "\n",
    "Collocations are phrases that occur together more often than would be expected by chance.\n",
    "\n",
    "- Non-compositional: the meaning is not the sum of the parts\n",
    "  > e.g., “New York” is not the sum of “New” and “York”\n",
    "- Non-substitutable: cannot substitute one component with synonyms\n",
    "  > e.g., “fast food” is not the same as “quick food”\n",
    "- Non-modifiable: cannot modify with additional words\n",
    "  > e.g., “kick around the bucket” is not the same as “kick the bucket around”\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22b6dbd5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Pointwise Mutual Information\n",
    "\n",
    "- Pointwise Mutual Information (PMI) is a measure of how often two words co-occur in a corpus.\n",
    "- PMI is defined as:\n",
    "\n",
    "$$\n",
    "PMI(w_1,w_2) = \\frac{P(w_1\\_ w_2)}{P(w_1)P(w_2)} \\\\\n",
    "= \\frac{\\text{Prob. of collocation, actual}}{\\text{Prob. of collocation, if independent}}\n",
    "$$\n",
    "\n",
    "where $w_1$ and $w_2$ are words in the vocabulary, and $w_1$, $w_2$ is the N-gram $w_1\\_w_2$.\n",
    "ranks words by how often they collocate, relative to how often they occur apart.\n",
    "\n",
    "- Generalizes to longer phrases (length N) as the geometric mean of the probabilities:\n",
    "\n",
    "$$ \\frac{P(w*1,\\ldots w_2 )}{\\prod*{i=1}^{n} \\sqrt[n]{P(w_i)}} $$\n",
    "\n",
    "- Caveat: Rare words will have high PMI, but this is not necessarily a good thing.\n",
    "  - Can use a threshold to filter out rare words.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5d01e0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Out-of-Vocabulary Words (OOV) for N-grams\n",
    "\n",
    "- OOV words are words that are not in the vocabulary.\n",
    "- OOV words are a problem for N-gram models.\n",
    "  - Can be replaced with a special token, e.g. `<UNK>`.\n",
    "  - Can be replaced with the POS tag, e.g. `<NOUN>`.\n",
    "  - Can be replaced with the hypernym, e.g. `<ANIMAL>`.\n",
    "  - Can use a hash function to map OOV words to a fixed number of buckets.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbfb95b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## What are parts of speech?\n",
    "\n",
    "- Nouns, Pronouns, Proper Nouns,\n",
    "- Verbs, Auxiliaries,\n",
    "- Adjectives, Adverbs\n",
    "- Prepositions, Conjunctions,\n",
    "- Determiners, Particles\n",
    "- Numerals, Symbols,\n",
    "- Interjections, etc.\n",
    "\n",
    "See e.g. https://universaldependencies.org/u/pos/\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fa4a7e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### POS Tagging\n",
    "\n",
    "Words often have more than one POS:\n",
    "\n",
    "- The back door `(adjective)`\n",
    "- On my back `(noun)`\n",
    "- Win the voters back `(particle)`\n",
    "- Promised to back the bill `(verb)`\n",
    "\n",
    "The POS tagging task:\n",
    "\n",
    "- Given a sequence of words, assign a POS tag to each word.\n",
    "- The POS tag is a label from a fixed set of tags.\n",
    "- Due to ambiguity (and unknown words), we cannot look up the POS tag in a dictionary.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bd29a23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Why POS Tagging?\n",
    "\n",
    "- POS tagging is one of the first steps in many NLP tasks.\n",
    "- For a traditional NLP pipeline, POS tagging is regarded as a prerequisite for further processing.\n",
    "  - Syntactic parsing: POS tags are used to build a parse tree.\n",
    "  - Information extraction: POS tags are used to identify named entities, relations, etc.\n",
    "- Although POS tagging is not a prerequisite for many modern NLP tasks, it is still useful.\n",
    "  - To understand the basic structure of a sentence.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ec49b84",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Creating a POS Tagger\n",
    "\n",
    "- A POS tagger is a classifier that assigns a POS tag to each word in a sentence.\n",
    "- To handle ambiguity, a POS tagger relies on learned models.\n",
    "- For a `new language or domain`, a POS tagger can be trained from scratch.\n",
    "  - Define a set of POS tags.\n",
    "  - Annotate a corpus with POS tags.\n",
    "- For an `existing language or domain`, a POS tagger can be trained on the existing annotated corpus.\n",
    "  - Obtain a corpus with POS tags.\n",
    "- To train a POS tagger,\n",
    "  - Choose a POS tagging algorithm. (e.g. HMM, CRF, etc.)\n",
    "  - Train the POS tagging algorithm on the annotated corpus.\n",
    "  - Evaluate the POS tagging algorithm on a test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7746afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7720f0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![](figs/21.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "756d750c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "[List of Universal POS tags](https://universaldependencies.org/u/pos/)\n",
    "\n",
    "![](figs/22.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0adb192",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-22 08:42:06.914835: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS => PROPN => NNP\n",
      "tagging => NOUN => NN\n",
      "is => AUX => VBZ\n",
      "the => DET => DT\n",
      "process => NOUN => NN\n",
      "of => ADP => IN\n",
      "marking => VERB => VBG\n",
      "up => ADP => RP\n",
      "a => DET => DT\n",
      "word => NOUN => NN\n",
      "in => ADP => IN\n",
      "a => DET => DT\n",
      "text => NOUN => NN\n",
      "( => PUNCT => -LRB-\n",
      "corpus => PROPN => NNP\n",
      ") => PUNCT => -RRB-\n",
      "as => ADP => IN\n",
      "corresponding => VERB => VBG\n",
      "to => ADP => IN\n",
      "a => DET => DT\n",
      "particular => ADJ => JJ\n",
      "part => NOUN => NN\n",
      "of => ADP => IN\n",
      "speech => NOUN => NN\n",
      ", => PUNCT => ,\n",
      "based => VERB => VBN\n",
      "on => ADP => IN\n",
      "both => CCONJ => CC\n",
      "its => PRON => PRP$\n",
      "definition => NOUN => NN\n",
      "and => CCONJ => CC\n",
      "its => PRON => PRP$\n",
      "context => NOUN => NN\n",
      ". => PUNCT => .\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download en\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"POS tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context.\"\n",
    "\n",
    "for token in nlp(text):\n",
    "    print(token.text, \"=>\", token.pos_, \"=>\", token.tag_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3056c32f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Dependency Parsing\n",
    "\n",
    "- Dependency parsing is the task of assigning a syntactic dependency to each word in a sentence.\n",
    "- In dependency parsing, dependency tags represent the grammatical function of a word in a sentence.\n",
    "\n",
    "For example, in the sentence “The quick brown fox jumps over the lazy dog”,\n",
    "\n",
    "- A dependency exists from the `fox` to the `brown` in which the `fox` acts as the head and the `brown` acts as the dependent or child.\n",
    "- This dependency is labeled `amod` (adjectival modifier).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97ed09aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "[Universal Dependency Relations](https://universaldependencies.org/u/dep/)\n",
    "\n",
    "![](figs/23.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1883121e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"c8e30886aa8a42ac814d6fb21a6f802b-0\" class=\"displacy\" width=\"1625\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">quick</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">brown</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">fox</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">jumps</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">over</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">lazy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">dog.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c8e30886aa8a42ac814d6fb21a6f802b-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c8e30886aa8a42ac814d6fb21a6f802b-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c8e30886aa8a42ac814d6fb21a6f802b-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c8e30886aa8a42ac814d6fb21a6f802b-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c8e30886aa8a42ac814d6fb21a6f802b-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c8e30886aa8a42ac814d6fb21a6f802b-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c8e30886aa8a42ac814d6fb21a6f802b-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c8e30886aa8a42ac814d6fb21a6f802b-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c8e30886aa8a42ac814d6fb21a6f802b-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c8e30886aa8a42ac814d6fb21a6f802b-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c8e30886aa8a42ac814d6fb21a6f802b-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,89.5 1445.0,89.5 1445.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c8e30886aa8a42ac814d6fb21a6f802b-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c8e30886aa8a42ac814d6fb21a6f802b-0-6\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c8e30886aa8a42ac814d6fb21a6f802b-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,266.5 L1287,254.5 1303,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c8e30886aa8a42ac814d6fb21a6f802b-0-7\" stroke-width=\"2px\" d=\"M945,264.5 C945,2.0 1450.0,2.0 1450.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c8e30886aa8a42ac814d6fb21a6f802b-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1450.0,266.5 L1458.0,254.5 1442.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "sent = \"The quick brown fox jumps over the lazy dog.\"\n",
    "displacy.render(nlp(sent), jupyter=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61d0c3e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Constituency Parsing\n",
    "\n",
    "- Constituency parsing is the task of analyzing a sentence by breaking it into sub-phrases (constituents).\n",
    "- These sub-phrases belong to a fixed set of syntactic categories, such as NP (noun phrase) and VP (verb phrase).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5e3fac7",
   "metadata": {},
   "source": [
    "> “It took me more than two hours to translate a few pages of English.”\n",
    "\n",
    "![](figs/27.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0299fad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
